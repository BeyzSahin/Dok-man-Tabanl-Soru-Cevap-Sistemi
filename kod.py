# -*- coding: utf-8 -*-
"""kod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJIae-34g1gDJ5Lg36YI-_EygcppZRaR
"""

import sys, subprocess, pkgutil, re
import numpy as np
from typing import List, Tuple
import torch

# ---------- 0) Kurulum ----------
def install_libs():
    pkgs = [
        "pypdf", "faiss-cpu", "sentence-transformers",
        "transformers", "accelerate", "gradio", "numpy",
        "python-docx"
    ]
    try:
        import torch
        if torch.cuda.is_available():
            pkgs.append("bitsandbytes")
    except:
        pass

    for pkg in pkgs:
        base = pkg.split("-")[0]
        if not pkgutil.find_loader(base):
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", pkg])

install_libs()

# ---------- 1) Importlar ----------
from pypdf import PdfReader
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import gradio as gr
from docx import Document

# ---------- 2) Ayarlar ----------
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LLM_NAME    = "Qwen/Qwen2.5-1.5B-Instruct"

HAS_CUDA = torch.cuda.is_available()
print("Sistem:", "GPU (CUDA)" if HAS_CUDA else "CPU")

embedder = SentenceTransformer(EMBED_MODEL)

quant_config = BitsAndBytesConfig(load_in_4bit=True) if HAS_CUDA else None
tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)
gen_model = AutoModelForCausalLM.from_pretrained(
    LLM_NAME,
    device_map="auto" if HAS_CUDA else None,
    torch_dtype=torch.float16 if HAS_CUDA else torch.float32,
    quantization_config=quant_config,
    low_cpu_mem_usage=True
)

# ---------- 3) Metin okuma/temizleme ----------
def clean_pdf_text(t: str) -> str:
    if not t:
        return ""
    t = t.replace("\r\n", "\n").replace("\r", "\n")
    t = re.sub(r"(\w)-\n(\w)", r"\1\2", t)          # heceleme düzelt
    t = re.sub(r"(?<!\n)\n(?!\n)", " ", t)          # tek newline -> space
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def read_docx(path: str) -> str:
    doc = Document(path)
    paras = []
    for p in doc.paragraphs:
        txt = (p.text or "").strip()
        if txt:
            paras.append(txt)
    return "\n\n".join(paras).strip()

def process_text(file_obj) -> str:
    if file_obj is None:
        return ""
    path = file_obj.name
    try:
        low = path.lower()
        if low.endswith(".pdf"):
            reader = PdfReader(path)
            pages = [(page.extract_text() or "") for page in reader.pages]
            text = clean_pdf_text("\n\n".join(pages))
        elif low.endswith(".docx"):
            text = read_docx(path)
            text = re.sub(r"[ \t]+", " ", text).strip()
        else:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
            text = re.sub(r"[ \t]+", " ", text)
            text = re.sub(r"\n{3,}", "\n\n", text).strip()
    except Exception as e:
        return f"Hata: {str(e)}"
    return text.strip()

# ---------- 4) Dil tespiti (doküman dili + soru dili) ----------
TR_CHARS = set("çğıöşüÇĞİÖŞÜ")

def has_turkish_chars(s: str) -> bool:
    return any(ch in TR_CHARS for ch in (s or ""))

def detect_doc_language(text: str) -> str:
    """
    Basit ama işe yarar:
    - Türkçe karakter barizse -> tr
    - Değilse ve metin çoğunlukla ASCII/İngilizce kelime görünümündeyse -> en
    """
    if not text:
        return "tr"
    sample = text[:6000]
    if has_turkish_chars(sample):
        return "tr"
    # İngilizce stopwords kaba kontrol
    en_hits = len(re.findall(r"\b(the|and|is|are|of|to|in|that|this|with|for|as)\b", sample.lower()))
    tr_hits = len(re.findall(r"\b(ve|ile|için|olarak|bu|şu|bir|da|de|nedir)\b", sample.lower()))
    if en_hits >= max(6, tr_hits + 3):
        return "en"
    return "tr"

def normalize_question(q: str) -> str:
    q = (q or "").strip()
    q = re.sub(r"\s+", " ", q)
    return q

def is_english_question(q: str) -> bool:
    qn = normalize_question(q).lower()
    en_keys = ["what is", "define", "explain", "briefly", "short", "long", "summarize", "detail"]
    return any(k in qn for k in en_keys)

def detect_output_lang(question: str, doc_lang: str) -> str:
    """
    KURAL:
    - Soru bariz İngilizceyse -> en
    - Soru bariz Türkçe ise (TR char / Türkçe anahtar) -> tr
    - Soru karışık/çok kısa ise: doküman dili neyse onu seç
    """
    q = normalize_question(question)
    q_low = q.lower()

    if is_english_question(q):
        return "en"

    if has_turkish_chars(q):
        return "tr"

    tr_keys = ["nedir", "açıkla", "özet", "detay", "kısaca", "uzun", "tanımla"]
    if any(k in q_low for k in tr_keys):
        return "tr"

    # soru çok kısaysa (ör. "AI?") -> doküman dili
    if len(q_low) <= 6:
        return doc_lang

    # varsayılan: doküman dili
    return doc_lang

def detect_length_mode(q: str) -> str:
    qn = normalize_question(q).lower()
    short_keys = ["kısa", "özet", "özetle", "kısaca", "short", "brief", "summarize"]
    long_keys  = ["uzun", "detay", "detaylı", "ayrıntı", "long", "detailed", "in depth", "elaborate"]
    if any(k in qn for k in long_keys): return "long"
    if any(k in qn for k in short_keys): return "short"
    return "medium"

def strip_control_phrases(q: str) -> str:
    qn = normalize_question(q).lower()
    junk = [
        "kısa açıkla","uzun açıkla","detaylı açıkla","özetle","kısaca","detaylandır",
        "kısa","uzun","detaylı","özet","ayrıntılı",
        "briefly","short","long","detailed","in depth","summarize","explain","define"
    ]
    for j in junk:
        qn = qn.replace(j, " ").strip()
    return re.sub(r"\s+", " ", qn).strip()

def question_terms(q: str) -> List[str]:
    qn = normalize_question(q).lower()
    toks = re.findall(r"[a-zA-ZçğıöşüÇĞİÖŞÜ0-9]+", qn)
    stop = set(["nedir","ne","what","is","the","a","an","and","or","to","of","in","on","for","ile","ve","da","de","mi","mı","mu","mü"])
    toks = [t for t in toks if t not in stop and len(t) >= 3]
    return toks[:12]

# ---------- 5) Cümle/Chunk yardımcıları ----------
def split_sentences(text: str) -> List[str]:
    txt = re.sub(r"\s+", " ", (text or "")).strip()
    if not txt:
        return []
    sents = re.split(r"(?<=[.!?])\s+", txt)
    return [s.strip() for s in sents if len(s.strip()) >= 20]

def truncate_to_sentence_end(text: str, max_chars: int) -> str:
    """Asla cümle ortasında bırakma."""
    t = re.sub(r"\s+", " ", (text or "")).strip()
    if not t:
        return ""
    if len(t) <= max_chars:
        return t if t.endswith((".", "!", "?")) else (t + ".")
    cut = t[:max_chars]
    last = max(cut.rfind("."), cut.rfind("!"), cut.rfind("?"))
    if last >= int(max_chars * 0.55):
        return cut[:last+1].strip()
    cut2 = cut.rsplit(" ", 1)[0].strip()
    return (cut2 + ".").strip()

def chunk_text_sentence_based(text: str, target_chars: int = 900, overlap_sents: int = 2) -> List[str]:
    sents = split_sentences(text)
    if not sents:
        return []
    chunks, cur, cur_len = [], [], 0
    for s in sents:
        if cur_len + len(s) + 1 <= target_chars:
            cur.append(s); cur_len += len(s) + 1
        else:
            chunks.append(" ".join(cur).strip())
            cur = cur[-overlap_sents:] if overlap_sents > 0 else []
            cur_len = sum(len(x) + 1 for x in cur)
            cur.append(s); cur_len += len(s) + 1
    if cur:
        chunks.append(" ".join(cur).strip())
    return [c for c in chunks if len(c) >= 140]

# ---------- 6) Özet (extractive, düzgün) ----------
def looks_like_header_or_meta(s: str) -> bool:
    sl = s.lower()
    bad = ["abstract", "özet", "keywords", "anahtar", "author", "doi", "issn", "journal", "conference", "copyright"]
    return any(b in sl for b in bad)

def extractive_summary_improved(full_text: str, max_sents: int = 6) -> str:
    sents = [s for s in split_sentences(full_text) if not looks_like_header_or_meta(s)]
    if not sents:
        return ""
    q = "Main definition, purpose, key methods, and key implications." if detect_doc_language(full_text) == "en" else \
        "Dokümanın ana fikri, tanım, amaç ve en önemli noktalar."
    q_emb = embedder.encode([q], normalize_embeddings=True)
    s_emb = embedder.encode(sents, normalize_embeddings=True)
    sims = (s_emb @ q_emb.T).reshape(-1)
    best = sorted(np.argsort(-sims)[:max_sents].tolist())
    out = " ".join(sents[i] for i in best).strip()
    return truncate_to_sentence_end(out, 850)

# ---------- 7) LLM ----------
def generate(system_prompt: str, user_prompt: str, max_new_tokens: int = 220, temp: float = 0.2) -> str:
    messages = [{"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(gen_model.device)

    do_sample = temp > 0.0
    with torch.no_grad():
        out = gen_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temp if do_sample else 0.0,
            top_p=0.9 if do_sample else 1.0,
            repetition_penalty=1.12,
            eos_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()

# ---------- 8) RAG kalite kilidi ----------
BAD_START_TR = ("ancak", "ayrıca", "bununla", "fakat", "lakin", "öte yandan", "diğer yandan", "bu nedenle", "dolayısıyla")
BAD_START_EN = ("however", "also", "moreover", "but", "yet", "on the other hand", "therefore", "thus")

def starts_bad(ans: str, out_lang: str) -> bool:
    a = (ans or "").strip().lower()
    if not a:
        return False
    starts = BAD_START_EN if out_lang == "en" else BAD_START_TR
    return any(a.startswith(s) for s in starts)

def strip_markdown_and_bullets(text: str) -> str:
    t = (text or "").strip()
    t = re.sub(r"^\s*#{1,6}\s+.*?$", "", t, flags=re.MULTILINE)
    t = re.sub(r"^\s*[-•*]\s+", "", t, flags=re.MULTILINE)
    t = re.sub(r"^\s*\d+\)\s+", "", t, flags=re.MULTILINE)
    t = re.sub(r"^\s*\d+\.\s+", "", t, flags=re.MULTILINE)
    t = re.sub(r"\n{2,}", "\n", t)
    t = re.sub(r"\s*\n\s*", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def topic_gate(question: str, chosen_chunks: List[str]) -> bool:
    terms = question_terms(question)
    if not terms:
        return True
    blob = " ".join(chosen_chunks).lower()
    hits = sum(1 for t in terms if t in blob)
    needed = 1 if len(terms) <= 5 else 2
    return hits >= needed

def evidence_sentences_answer(question: str, chosen_chunks: List[str], out_lang: str, mode: str = "medium") -> str:
    """
    1) Kaynak cümlelerinden seçerek taslak oluşturur (sadakat ↑, denklik ↑)
    2) out_lang -> Türkçe/İngilizce "yok" mesajı
    """
    if mode == "short":
        max_sents = 2; max_chars = 420
    elif mode == "long":
        max_sents = 6; max_chars = 920
    else:
        max_sents = 4; max_chars = 760

    sents = []
    for ch in chosen_chunks:
        sents.extend(split_sentences(ch))

    seen, uniq = set(), []
    for s in sents:
        key = re.sub(r"\W+", "", s.lower())
        if key not in seen:
            seen.add(key)
            uniq.append(s)

    if not uniq:
        return "This information is not available in the document." if out_lang == "en" else "Bu bilgi dokümanda yer almıyor."

    q_emb = embedder.encode([question], normalize_embeddings=True)
    s_emb = embedder.encode(uniq, normalize_embeddings=True)
    sims = (s_emb @ q_emb.T).reshape(-1)

    best = np.argsort(-sims)[:max_sents]
    best_sorted = sorted(best.tolist())
    out = " ".join(uniq[i] for i in best_sorted).strip()

    out = strip_markdown_and_bullets(out)
    out = truncate_to_sentence_end(out, max_chars)
    return out

def rag_rewrite_clean(question: str, draft: str, chosen_chunks: List[str], out_lang: str, mode: str) -> str:
    """
    Draft'ı sadece akıcılaştırır.
    KRİTİK: ÇIKIŞ DİLİ out_lang ile sabitlenir (EN ise EN).
    """
    if mode == "short":
        style = "One paragraph, 2-3 sentences." if out_lang == "en" else "Tek paragraf, 2-3 cümle."
        max_tok = 160
    elif mode == "long":
        style = "One paragraph, 6-8 sentences." if out_lang == "en" else "Tek paragraf, 6-8 cümle."
        max_tok = 270
    else:
        style = "One paragraph, 4-5 sentences." if out_lang == "en" else "Tek paragraf, 4-5 cümle."
        max_tok = 210

    context = "\n".join([f"[{i+1}] {truncate_to_sentence_end(c, 520)}" for i, c in enumerate(chosen_chunks)])

    if out_lang == "en":
        sys_prompt = (
            "You rewrite text strictly.\n"
            "Rules:\n"
            "1) Do NOT add any new information.\n"
            "2) Output must be ONE paragraph. No headings, no bullets, no markdown.\n"
            "3) Start directly with a definition sentence. Do NOT start with conjunctions like 'However/Also'.\n"
            "4) Keep wording close to the sources.\n"
            "5) Output language MUST be English.\n"
        )
        user_prompt = (
            f"Question: {question}\n"
            f"Style: {style}\n\n"
            f"Sources:\n{context}\n\n"
            f"Text:\n{draft}\n\n"
            "Rewrite more fluently (same meaning, English only):"
        )
    else:
        sys_prompt = (
            "Sen sadece yeniden yazarsın.\n"
            "Kurallar:\n"
            "1) Yeni bilgi EKLEME.\n"
            "2) TEK PARAGRAF. Başlık yok, madde yok, markdown yok.\n"
            "3) Doğrudan TANIM cümlesiyle başla. 'Ancak/Ayrıca/Fakat' ile BAŞLAMA.\n"
            "4) İfadeleri kaynağa yakın tut.\n"
            "5) Çıktı dili MUTLAKA Türkçe olacak.\n"
        )
        user_prompt = (
            f"Soru: {question}\n"
            f"Yazım: {style}\n\n"
            f"Kaynak:\n{context}\n\n"
            f"Metin:\n{draft}\n\n"
            "Daha akıcı hale getir (aynı anlam, sadece Türkçe):"
        )

    out = generate(sys_prompt, user_prompt, max_new_tokens=max_tok, temp=0.0)
    out = strip_markdown_and_bullets(out)
    out = truncate_to_sentence_end(out, 900 if mode != "short" else 450)

    if starts_bad(out, out_lang):
        return draft
    return out

def similarity_answer_to_context(answer: str, context_chunks: List[str]) -> float:
    """Cevap↔Kaynak denklik (cümle bazlı)."""
    ans_sents = split_sentences(answer)
    if not ans_sents or not context_chunks:
        return 0.0
    ctx_sents = []
    for ch in context_chunks:
        ctx_sents.extend(split_sentences(ch))
    ctx_sents = [s for s in ctx_sents if len(s) >= 20]
    if not ctx_sents:
        return 0.0

    a_emb = embedder.encode(ans_sents, normalize_embeddings=True)
    c_emb = embedder.encode(ctx_sents, normalize_embeddings=True)
    sims = (a_emb @ c_emb.T)
    best = np.max(sims, axis=1)
    return float(np.clip(np.mean(best) * 100.0, 0.0, 100.0))

def mmr_select(query_emb: np.ndarray, cand_embs: np.ndarray, k: int, lam: float = 0.65) -> List[int]:
    n = cand_embs.shape[0]
    if n == 0:
        return []
    sims_to_q = (cand_embs @ query_emb.reshape(-1, 1)).reshape(-1)
    selected, remaining = [], list(range(n))

    first = int(np.argmax(sims_to_q))
    selected.append(first)
    remaining.remove(first)

    while len(selected) < min(k, n) and remaining:
        best_idx, best_score = None, -1e9
        for r in remaining:
            sim_q = float(sims_to_q[r])
            sim_sel = max(float(cand_embs[r] @ cand_embs[s]) for s in selected) if selected else 0.0
            score = lam * sim_q - (1.0 - lam) * sim_sel
            if score > best_score:
                best_score, best_idx = score, r
        selected.append(best_idx)
        remaining.remove(best_idx)

    return selected

# ---------- 9) RAG sistemi ----------
class StrictRAG:
    def __init__(self):
        self.index = None
        self.chunks: List[str] = []
        self.chunk_embs = None
        self.full_text = ""
        self.doc_lang = "tr"  # 'tr' or 'en'

    def load_doc(self, file):
        if file is None:
            return "Dosya yok.", "", "", 0, ""

        self.full_text = process_text(file)
        if (not self.full_text) or self.full_text.startswith("Hata:"):
            return "Doküman okunamadı.", "", "", 0, ""

        self.doc_lang = detect_doc_language(self.full_text)

        self.chunks = chunk_text_sentence_based(self.full_text, target_chars=900, overlap_sents=2)
        if not self.chunks:
            return "Metin boş / chunk üretilemedi.", "", "", 0, self.doc_lang

        self.chunk_embs = embedder.encode(self.chunks, normalize_embeddings=True).astype("float32")
        d = self.chunk_embs.shape[1]
        self.index = faiss.IndexFlatIP(d)
        self.index.add(self.chunk_embs)

        summary = extractive_summary_improved(self.full_text, max_sents=6)
        preview = truncate_to_sentence_end(self.full_text, 1100)

        status = f"✅ Hazır! {len(self.chunks)} parça indekslendi."
        return status, preview, summary, len(self.chunks), self.doc_lang

    def retrieve(self, question: str, top_k: int = 8) -> Tuple[List[int], List[float]]:
        q_emb = embedder.encode([question], normalize_embeddings=True).astype("float32")
        D, I = self.index.search(q_emb, top_k)
        idxs = I[0].tolist()
        sims = D[0].tolist()
        idxs = [i for i in idxs if i >= 0]
        return idxs, sims

    def expand_neighbors(self, idxs: List[int]) -> List[int]:
        expanded = []
        for i in idxs:
            expanded.append(i)
            if i - 1 >= 0: expanded.append(i - 1)
            if i + 1 < len(self.chunks): expanded.append(i + 1)
        seen, out = set(), []
        for x in expanded:
            if x not in seen:
                seen.add(x); out.append(x)
        return out

    def evidence_check(self, sims: List[float], min_top1: float = 0.42, min_avg3: float = 0.36) -> bool:
        if not sims:
            return False
        top1 = sims[0]
        avg3 = float(np.mean(sims[:min(3, len(sims))]))
        return (top1 >= min_top1) and (avg3 >= min_avg3)

    def query(self, question: str, top_k: int = 8, rewrite: bool = True):
        if not self.index:
            return "Önce dosya yükleyin.", "", "", "", ""

        mode = detect_length_mode(question)
        clean_q = strip_control_phrases(question) or normalize_question(question).lower()
        out_lang = detect_output_lang(question, self.doc_lang)

        # ✅ Dokümansız cevap (dil sabit)
        if out_lang == "en":
            gen_sys = "You are a helpful assistant. Answer clearly in English only."
        else:
            gen_sys = "Sen yardımcı bir asistansın. Cevabı sadece Türkçe ver."
        gen_ans = generate(gen_sys, question, max_new_tokens=220, temp=0.2)
        gen_ans = strip_markdown_and_bullets(gen_ans)
        gen_ans = truncate_to_sentence_end(gen_ans, 950)

        idxs, sims = self.retrieve(clean_q, top_k=top_k)
        raw_retrieved = [self.chunks[i] for i in idxs] if idxs else []

        if not self.evidence_check(sims):
            sources = "\n\n---\n\n".join([f"Skor: {s:.3f}\n{truncate_to_sentence_end(t, 650)}" for s, t in zip(sims[:5], raw_retrieved[:5])])
            if out_lang == "en":
                rag_empty = "This information is not available in the document."
                metrics = (
                    f"Evidence: NO\nTop-1 Retrieval: {sims[0] if sims else 0:.3f}\n"
                    f"Groundedness: 0.00%\nSuccess: 0.00%\nMode: {mode}\nDocLang: {self.doc_lang}\nOutLang: {out_lang}"
                )
            else:
                rag_empty = "Bu bilgi dokümanda yer almıyor."
                metrics = (
                    f"Kanıt Yeterliliği: HAYIR\nTop-1 Retrieval Skoru: {sims[0] if sims else 0:.3f}\n"
                    f"Denklik (Cevap↔Doküman): %0.00\nBaşarı Oranı: %0.00\nMod: {mode}\nDokümanDili: {self.doc_lang}\nÇıktıDili: {out_lang}"
                )
            return gen_ans, rag_empty, "%0.00", metrics, sources

        # MMR ile 4 parça seç
        idxs_exp = self.expand_neighbors(idxs)
        cand_embs = self.chunk_embs[idxs_exp]
        q_emb = embedder.encode([clean_q], normalize_embeddings=True).astype("float32")[0]
        mmr_local = mmr_select(q_emb, cand_embs, k=min(4, len(idxs_exp)), lam=0.65)
        chosen_idxs = [idxs_exp[i] for i in mmr_local]
        chosen_chunks = [self.chunks[i] for i in chosen_idxs]

        if not topic_gate(clean_q, chosen_chunks):
            sources = "\n\n---\n\n".join(
                [f"Top-1 Skor: {sims[0]:.3f}\n(İlk bulunan parça)\n{truncate_to_sentence_end(raw_retrieved[0] if raw_retrieved else '', 700)}"]
                + [f"Seçilen Parça {i+1}:\n{truncate_to_sentence_end(c, 700)}" for i, c in enumerate(chosen_chunks)]
            )
            rag_empty = "This information is not available in the document." if out_lang == "en" else "Bu bilgi dokümanda yer almıyor."
            if out_lang == "en":
                metrics = (
                    f"Evidence: NO (Topic mismatch)\nTop-1 Retrieval: {sims[0]:.3f}\n"
                    f"Groundedness: 0.00%\nSuccess: 0.00%\nMode: {mode}\nDocLang: {self.doc_lang}\nOutLang: {out_lang}"
                )
            else:
                metrics = (
                    f"Kanıt Yeterliliği: HAYIR (Konu uyuşmazlığı)\nTop-1 Retrieval Skoru: {sims[0]:.3f}\n"
                    f"Denklik (Cevap↔Doküman): %0.00\nBaşarı Oranı: %0.00\nMod: {mode}\nDokümanDili: {self.doc_lang}\nÇıktıDili: {out_lang}"
                )
            return gen_ans, rag_empty, "%0.00", metrics, sources

        # ✅ RAG: önce kaynak cümlelerinden taslak (sadakat ↑)
        rag_draft = evidence_sentences_answer(question, chosen_chunks, out_lang=out_lang, mode=mode)

        # ✅ sadece akıcılaştır (yeni bilgi yok, dil sabit)
        rag_ans = rag_draft
        if rewrite and ("not available" not in rag_draft.lower()) and ("dokümanda yer almıyor" not in rag_draft.lower()):
            rag_ans = rag_rewrite_clean(question, rag_draft, chosen_chunks, out_lang=out_lang, mode=mode)

        rag_ans = strip_markdown_and_bullets(rag_ans)
        rag_ans = truncate_to_sentence_end(rag_ans, 900 if mode != "short" else 450)

        # anti "however/ancak" başlangıç
        if starts_bad(rag_ans, out_lang):
            # draft'a geri dön
            rag_ans = rag_draft
            rag_ans = strip_markdown_and_bullets(rag_ans)
            rag_ans = truncate_to_sentence_end(rag_ans, 900 if mode != "short" else 450)

        denklik = similarity_answer_to_context(rag_ans, chosen_chunks)

        top1_pct = float(np.clip(sims[0], 0.0, 1.0)) * 100.0
        avg3 = float(np.mean(sims[:min(3, len(sims))]))
        avg3_pct = float(np.clip(avg3, 0.0, 1.0)) * 100.0

        # ✅ başarı oranı: denklik ana, retrieval destek (stabil)
        success = float(np.clip(0.70 * denklik + 0.20 * avg3_pct + 0.10 * top1_pct, 0.0, 100.0))

        sources = "\n\n---\n\n".join(
            [f"Top-1 Skor: {sims[0]:.3f}\n(İlk bulunan parça)\n{truncate_to_sentence_end(raw_retrieved[0] if raw_retrieved else '', 700)}"]
            + [f"Seçilen Parça {i+1}:\n{truncate_to_sentence_end(c, 700)}" for i, c in enumerate(chosen_chunks)]
        )

        if out_lang == "en":
            metrics = (
                f"Evidence: YES\n"
                f"Top-1 Retrieval: {sims[0]:.3f}\n"
                f"Avg-3 Retrieval: {avg3:.3f}\n"
                f"Groundedness (Answer↔Doc): {denklik:.2f}%\n"
                f"Success: {success:.2f}%\n"
                f"Mode: {mode}\nDocLang: {self.doc_lang}\nOutLang: {out_lang}"
            )
        else:
            metrics = (
                f"Kanıt Yeterliliği: EVET\n"
                f"Top-1 Retrieval Skoru: {sims[0]:.3f}\n"
                f"Avg-3 Retrieval Skoru: {avg3:.3f}\n"
                f"Denklik (Cevap↔Doküman): %{denklik:.2f}\n"
                f"Başarı Oranı: %{success:.2f}\n"
                f"Mod: {mode}\nDokümanDili: {self.doc_lang}\nÇıktıDili: {out_lang}"
            )

        return gen_ans, rag_ans, f"%{success:.2f}", metrics, sources

# ---------- 10) UI (Turuncu Tema + Başlık düzeltildi) ----------
rag = StrictRAG()

css = """
/* Genel arkaplan */
.gradio-container { background: #0b0f14 !important; }

/* Başlık/etiket rengi */
label, .prose h1, .prose h2, .prose h3, .prose p { color: #ffb020 !important; }

/* Input kutuları */
textarea, input, .gr-box, .gr-panel {
  background: #0f1720 !important;
  color: #e6edf3 !important;
  border: 1px solid rgba(255,176,32,0.25) !important;
}

/* Butonlar */
.gr-button.primary {
  background: #ff8a00 !important;
  color: #0b0f14 !important;
  border: none !important;
  font-weight: 700 !important;
}
.gr-button.primary:hover { filter: brightness(1.05); }

/* Slider & checkbox */
input[type="range"] { accent-color: #ff8a00; }
input[type="checkbox"] { accent-color: #ff8a00; }

/* Kart görünümü */
.gr-box, .gr-panel {
  border-radius: 14px !important;
  box-shadow: 0 0 0 1px rgba(255,176,32,0.10) inset !important;
}

/* Markdown */
.prose a { color: #ffb020 !important; }
"""

with gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:
    gr.Markdown(
        "# Doküman Tabanlı Soru–Cevap Sistemi \n"
        "PDF/TXT/DOCX yükle → dokümansız + dokümana dayalı cevap al. "
    )

    with gr.Row():
        f_in = gr.File(label=" Doküman Yükle (PDF / TXT / DOCX)")
        btn_load = gr.Button(" Yükle ve Analiz Et", variant="primary")

    with gr.Row():
        stat = gr.Textbox(label="Durum", interactive=False)
        chunk_c = gr.Number(label="Parça Sayısı", interactive=False)
        doc_lang_box = gr.Textbox(label="Doküman Dili ", interactive=False)

    with gr.Row():
        prev = gr.Textbox(label="Önizleme ", lines=6, interactive=False)
        summ = gr.Textbox(label="Özet ", lines=6, interactive=False)

    btn_load.click(rag.load_doc, [f_in], [stat, prev, summ, chunk_c, doc_lang_box])

    gr.Markdown("---")

    with gr.Row():
        q_in = gr.Textbox(
            label=" Soru (Türkçe/İngilizce)",
            placeholder=" "
        )

    with gr.Row():
        k_sl = gr.Slider(4, 12, 8, step=1, label="Retrieval Top-K (öneri: 8)")
        rewrite_chk = gr.Checkbox(value=True, label="RAG cevabını akıcılaştır.")

    btn_ask = gr.Button(" Cevapla", variant="primary")

    with gr.Row():
        ans_gen = gr.Textbox(label=" Dokümansız Cevap (Genel)", lines=8)
        ans_rag = gr.Textbox(label=" STRICT RAG Cevabı ", lines=8)

    with gr.Row():
        score_box = gr.Textbox(label=" Başarı Oranı (%)")
        met_box = gr.Textbox(label=" Metrikler", lines=9)
        src_box = gr.Textbox(label=" Kaynaklar / Seçilen Parçalar", lines=14)

    btn_ask.click(rag.query, [q_in, k_sl, rewrite_chk], [ans_gen, ans_rag, score_box, met_box, src_box])

demo.launch(debug=False)